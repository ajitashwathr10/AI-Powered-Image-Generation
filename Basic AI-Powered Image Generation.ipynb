{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Import modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "from tensorflow,keras.preprocessing import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_models(clip_model = \"openai/clip-vit-base-patch32\", inception_weights = \"imagenet\"):\n",
        "    processor = CLIPProcessor.from_pretrained(clip_model)\n",
        "    clip_model = CLIPModel.from_pretrained(clip_model).to(DEVICE)\n",
        "    inception_model = inception_v3.InceptionV(weights = inception_weights, include_top = False)\n",
        "    return processor, clip_model, inception_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_image(img_path, target_size = (300, 300)):\n",
        "    img = load_img(img_path, target_size = target_size)\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    return img_tensor.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Function - Generate a virtual image\n",
        "def virtual_image(base_img_path, text_prompt, iterations = 10):\n",
        "    base_img = preprocess_img(base_img_path)\n",
        "    virtual_img = generate_noise(base_img.shape)\n",
        "    inputs = processer(text_prompt, return_tensors = 'pt')\n",
        "    with torch.no_grad():\n",
        "        text_features = model(**inputs).pooler_output\n",
        "    for i in range(iterations):\n",
        "        virutal_features = inception_model.predict(virtual_img)\n",
        "        #Calculate the loss function\n",
        "        loss = np.mean(virtual_features) - np.dot(text_features.squeeze(), virtual_features.mean(axis = (1, 2)))\n",
        "        #Calculate the gradient descent\n",
        "        grad_des = np.gradient(loss)[0]\n",
        "        octa = np.stack([grad_des,(grad_des[:, :, 1:] + grad_des[:, :, :-1]) / 2, grad_des[:, :, :-2] + grad_des[:, :, 1:]])\n",
        "        virtual_img += octa * 0.1\n",
        "    #De-processing the virtual image\n",
        "    virtual_img[:, :, 0] += 1.\n",
        "    virtual_img[:, :, 1] += 1.\n",
        "    virtual_img[:, :, 2] += 1.\n",
        "    virtual_img *= 127.5\n",
        "    virtual_img = np.clip(virtual_img, 0, 255).astype('uint8')\n",
        "    #Combining virtual image with the org one\n",
        "    base_img = cv2.imread(base_img_path)\n",
        "    virtual_img = cv2.resize(virtual_img, (base_img.shape[1], base_img.shape[0]))\n",
        "    alpha = 0.5\n",
        "    blended_img = cv2.addWeighted(base_img, alpha, virtual_img, 1 - alpha, 0)\n",
        "    return blended_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Main\n",
        "base_img = \"\" #Path of the file\n",
        "text_prompt = input(\"Enter what kind of image you want: \")\n",
        "virtual = virtual_image(base_img, text_prompt, iterations = 20)\n",
        "cv2.inwrite(\"generated_image.jpg\", virtual)\n",
        "#Done"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
