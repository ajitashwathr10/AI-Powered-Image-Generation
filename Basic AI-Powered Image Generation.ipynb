{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Import modules\nfrom tensorflow.keras.applications import inception_v3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.random import normal\nimport numpy as np\nimport cv2\nfrom transformers import CLIPProcessor, CLIPModel",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Load pre-trained models\ninc_model = inception_v3.InceptionV3(weights='imagenet', include_top=False)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Function - Preprocess an image\ndef preprocess_img(img_path):\n    img = load_img(img_path, target_size = (300, 300))\n    x = img_to_array(img)\n    y = np.expand_dims(x, axis = 0)\n    x = y\n    x = x / 127.5 - 1.\n    #Scales the pixel intensity range from 0 - 255 to a range centered around 0\n    #Positive values: Brighter pixels\n    #Negative values: Darker pixels\n    return x",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Function - Generate image noise\ndef generate_noise(shape):\n    return normal(0, 1, size = shape).astype(\"float32\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Function - Generate a virtual image\ndef virtual_image(base_img_path, text_prompt, iterations = 10):\n    base_img = preprocess_img(base_img_path)\n    virtual_img = generate_noise(base_img.shape)\n    inputs = processer(text_prompt, return_tensors = 'pt')\n    with torch.no_grad():\n        text_features = model(**inputs).pooler_output\n    for i in range(iterations):\n        virutal_features = inception_model.predict(virtual_img)\n        #Calculate the loss function\n        loss = np.mean(virtual_features) - np.dot(text_features.squeeze(), virtual_features.mean(axis = (1, 2)))\n        #Calculate the gradient descent\n        grad_des = np.gradient(loss)[0]\n        octa = np.stack([grad_des,(grad_des[:, :, 1:] + grad_des[:, :, :-1]) / 2, grad_des[:, :, :-2] + grad_des[:, :, 1:]])\n        virtual_img += octa * 0.1\n    #De-processing the virtual image\n    virtual_img[:, :, 0] += 1.\n    virtual_img[:, :, 1] += 1.\n    virtual_img[:, :, 2] += 1.\n    virtual_img *= 127.5\n    virtual_img = np.clip(virtual_img, 0, 255).astype('uint8')\n    #Combining virtual image with the org one\n    base_img = cv2.imread(base_img_path)\n    virtual_img = cv2.resize(virtual_img, (base_img.shape[1], base_img.shape[0]))\n    alpha = 0.5\n    blended_img = cv2.addWeighted(base_img, alpha, virtual_img, 1 - alpha, 0)\n    return blended_img",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Main\nbase_img = \"\" #Path of the file\ntext_prompt = input(\"Enter what kind of image you want: \")\nvirtual = virtual_image(base_img, text_prompt, iterations = 20)\ncv2.inwrite(\"generated_image.jpg\", virtual)\n#Done",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}